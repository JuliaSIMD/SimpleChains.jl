<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Adding a custom loss layer · SimpleChains.jl</title><script data-outdated-warner src="../../assets/warner.js"></script><link rel="canonical" href="https://PumasAI.github.io/SimpleChains.jl/examples/custom_loss_layer/"/><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.045/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.24/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../../">SimpleChains.jl</a></span></div><form class="docs-search" action="../../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../../">Home</a></li><li><span class="tocitem">Examples</span><ul><li><a class="tocitem" href="../smallmlp/">Small Multi-Layer Perceptron</a></li><li><a class="tocitem" href="../mnist/">MNIST - Convolutions</a></li><li class="is-active"><a class="tocitem" href>Adding a custom loss layer</a><ul class="internal"><li><a class="tocitem" href="#Mathematical-background-for-a-Binary-Cross-Entropy-Loss"><span>Mathematical background for a Binary Cross Entropy Loss</span></a></li><li><a class="tocitem" href="#Implementing-a-custom-loss-type"><span>Implementing a custom loss type</span></a></li></ul></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Examples</a></li><li class="is-active"><a href>Adding a custom loss layer</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Adding a custom loss layer</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/PumasAI/SimpleChains.jl/blob/main/docs/src/examples/custom_loss_layer.md#" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Adding-a-custom-loss-layer"><a class="docs-heading-anchor" href="#Adding-a-custom-loss-layer">Adding a custom loss layer</a><a id="Adding-a-custom-loss-layer-1"></a><a class="docs-heading-anchor-permalink" href="#Adding-a-custom-loss-layer" title="Permalink"></a></h1><p>Loss functions like the <code>LogitCrossEntropyLoss</code> are defined for users to be able to quickly prototype models on new problems. However, sometimes there is a need to write one&#39;s own customized loss function. This example will walk through this process.</p><p>To show which functions need to be implemented for your own custom loss, this example will walk through implementing a <code>BinaryLogitCrossEntropyLoss</code>, which acts on a model with only a single output, and binary targets.</p><h2 id="Mathematical-background-for-a-Binary-Cross-Entropy-Loss"><a class="docs-heading-anchor" href="#Mathematical-background-for-a-Binary-Cross-Entropy-Loss">Mathematical background for a Binary Cross Entropy Loss</a><a id="Mathematical-background-for-a-Binary-Cross-Entropy-Loss-1"></a><a class="docs-heading-anchor-permalink" href="#Mathematical-background-for-a-Binary-Cross-Entropy-Loss" title="Permalink"></a></h2><p>Consider the following model:</p><p class="math-container">\[p_\theta(X_i) = \sigma (f(X_i)),\]</p><p>where <span>$X_i$</span> is the input features, <span>$\sigma$</span> is the sigmoid function, given by <span>$\sigma(x)=(1+e^{-x})^{-1}$</span> and <span>$f_\theta$</span> is some function mapping defined by your model, which is parameterized by parameters <span>$\theta$</span>. The output of <span>$f_\theta (X_i)$</span> is called the &quot;logit&quot;. The loss function we want to calculate is the following:</p><p class="math-container">\[L(\theta| X, Y) = -\sum_i \left [ Y_i\ln{p_\theta (X_i)} + (1-Y_i)\ln{(1-p_\theta (X_i))} \right ],\]</p><p>where <span>$Y_i$</span> is the true binary label of the <span>$i^\text{th}$</span> sample. In order to implement this custom loss, we have to know what the gradient of this loss function is, w.r.t the parameters:</p><p class="math-container">\[    \frac{{\partial }}{{\partial } \theta} L(\theta | X, Y) = -\sum_i  Y_i\frac{{\partial }}{{\partial } \theta}\ln{p_\theta (X_i)} + (1-Y_i)\frac{{\partial }}{{\partial } \theta}\ln (1-p_\theta (X_i)).\]</p><p>To simplify this calculation, we can use the fact that <span>$1-p_\theta (x)=p_\theta (-x)$</span>, and <span>$\frac{{\partial }}{{\partial } \theta}\ln(p_\theta(X_i))=(1+e^{f_\theta(X_i)})^{-1} \frac{{\partial }}{{\partial } \theta} f_\theta(X_i)$</span>. We are left with:</p><p class="math-container">\[\frac{{\partial }}{{\partial } \theta} L(\theta| X, Y) = -\sum_i \left [ (2Y_i - 1){\left (1+e^{(2Y_i-1)f_\theta(X_i)} \right )}^{-1} \right ] \frac{{\partial }}{{\partial } \theta} f_\theta(X_i).\]</p><p>We have managed to write the derivative of the loss function, in terms of the derivative of the model, independently for each sample. The important part of this equation is the multiplicand of the partial derivative term; this term is the partial gradient used for back-propagation. From this point, we can begin writing the code.</p><h2 id="Implementing-a-custom-loss-type"><a class="docs-heading-anchor" href="#Implementing-a-custom-loss-type">Implementing a custom loss type</a><a id="Implementing-a-custom-loss-type-1"></a><a class="docs-heading-anchor-permalink" href="#Implementing-a-custom-loss-type" title="Permalink"></a></h2><p>We start by importing <code>SimpleChains.jl</code> into the current namespace:</p><pre><code class="language-julia hljs">using SimpleChains</code></pre><p>We can now define our own type, which is a subtype of <code>SimpleChains.AbstractLoss</code>:</p><pre><code class="language-julia hljs">struct BinaryLogitCrossEntropyLoss{T,Y&lt;:AbstractVector{T}} &lt;: SimpleChains.AbstractLoss{T}
    targets::Y
end</code></pre><p>The function used to get the inner targets is called <code>target</code> and can be defined easily:</p><pre><code class="language-julia hljs">SimpleChains.target(loss::BinaryLogitCrossEntropyLoss) = loss.targets
(loss::BinaryLogitCrossEntropyLoss)(x::AbstractArray) = BinaryLogitCrossEntropyLoss(x)</code></pre><p>Next, we define how to calculate the loss, given some logits:</p><pre><code class="language-julia hljs">function calculate_loss(loss::BinaryLogitCrossEntropyLoss, logits)
    y = loss.targets
    total_loss = zero(eltype(logits))
    for i in eachindex(y)
        p_i = inv(1 + exp(-logits[i]))
        y_i = y[i]
        total_loss -= y_i * log(p_i) + (1 - y_i) * (1 - log(p_i))
    end
    total_loss
end
function (loss::BinaryLogitCrossEntropyLoss)(previous_layer_output::AbstractArray{T}, p::Ptr, pu) where {T}
    total_loss = calculate_loss(loss, previous_layer_output)
    total_loss, p, pu
end</code></pre><p>As the other loss functions do this, we should define some functions to say that we don&#39;t want any preallocated temporary arrays:</p><pre><code class="language-julia hljs">function SimpleChains.layer_output_size(::Val{T}, sl::BinaryLogitCrossEntropyLoss, s::Tuple) where {T}
    SimpleChains._layer_output_size_no_temp(Val{T}(), sl, s)
end
function SimpleChains.forward_layer_output_size(::Val{T}, sl::BinaryLogitCrossEntropyLoss, s) where {T}
    SimpleChains._layer_output_size_no_temp(Val{T}(), sl, s)
end</code></pre><p>Finally, we define how to back-propagate the gradient from this loss function:</p><pre><code class="language-julia hljs">function SimpleChains.chain_valgrad!(
    __,
    previous_layer_output::AbstractArray{T},
    layers::Tuple{BinaryLogitCrossEntropyLoss},
    _::Ptr,
    pu::Ptr{UInt8},
) where {T}
    loss = getfield(layers, 1)
    total_loss = calculate_loss(loss, previous_layer_output)
    y = loss.targets

    # Store the backpropagated gradient in the previous_layer_output array.
    for i in eachindex(y)
        sign_arg = 2 * y[i] - 1
        # Get the value of the last logit
        logit_i = previous_layer_output[i]
        previous_layer_output[i] = -(sign_arg * inv(1 + exp(sign_arg * logit_i)))
    end

    return total_loss, previous_layer_output, pu
end</code></pre><p>That&#39;s all! The way we can now use this loss function, just like any other:</p><pre><code class="language-julia hljs">using SimpleChains

model = SimpleChain(
    static(2),
    TurboDense(tanh, 32),
    TurboDense(tanh, 16),
    TurboDense(identity, 1)
)

batch_size = 64
X = rand(Float32, 2, batch_size)
Y = rand(Bool, batch_size)

parameters = SimpleChains.init_params(model);
gradients = SimpleChains.alloc_threaded_grad(model);

# Add the loss like any other loss type
model_loss = SimpleChains.add_loss(model, BinaryLogitCrossEntropyLoss(Y));

SimpleChains.valgrad!(gradients, model_loss, X, parameters)</code></pre><p>Or alternatively, if you want to just train the parameters in full:</p><pre><code class="language-julia hljs">epochs = 100
SimpleChains.train_unbatched!(gradients, parameters, model_loss, X, SimpleChains.ADAM(), epochs); </code></pre></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../mnist/">« MNIST - Convolutions</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.24 on <span class="colophon-date" title="Tuesday 16 May 2023 16:22">Tuesday 16 May 2023</span>. Using Julia version 1.9.0.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
